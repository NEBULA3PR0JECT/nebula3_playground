{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/notebooks/pipenv\")\n",
    "sys.path.insert(0, \"/notebooks/nebula3_database\")\n",
    "sys.path.insert(0, \"/notebooks/\")\n",
    "from PIL import Image\n",
    "import requests\n",
    "import visual_genome.local as vg\n",
    "import json\n",
    "import copy\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import spacy\n",
    "import nltk\n",
    "import openai\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from database.arangodb import DatabaseConnector\n",
    "from config import NEBULA_CONF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe(\"spacy_wordnet\", after='tagger', config={'lang': nlp.lang})\n",
    "\n",
    "with open('/storage/keys/openai.key','r') as f:\n",
    "    OPENAI_API_KEY = f.readline().strip()\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "VG_DATA = '/storage/vg_data'\n",
    "IPC_COLLECTION = 'ipc_relations_spice'\n",
    "RECALL_COLLECTION = 'ipc_recall_spice'\n",
    "GLOBAL_TOKENS_COLLECTION = 's3_global_tokens'\n",
    "\n",
    "class PIPELINE:\n",
    "    def __init__(self):\n",
    "        config = NEBULA_CONF()\n",
    "        self.db_host = config.get_database_host()\n",
    "        self.database = config.get_playground_name()\n",
    "        self.gdb = DatabaseConnector()\n",
    "        self.db = self.gdb.connect_db(self.database)\n",
    "\n",
    "pipeline = PIPELINE()\n",
    "def get_sc_graph(id):\n",
    "    return vg.get_scene_graph(id, images=VG_DATA,\n",
    "                    image_data_dir=VG_DATA+'/by-id/',\n",
    "                    synset_file=VG_DATA+'/synsets.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTBaseGenerator:\n",
    "    def __init__(self):\n",
    "        self.pipeline = PIPELINE()\n",
    "        self.ipc_data = json.load(open('/storage/ipc_data/paragraphs_v1.json','r'))\n",
    "        self.global_captioner = 'blip'\n",
    "        self.global_tagger = 'blip'\n",
    "        self.places_source = 'blip'\n",
    "        self.global_prompt1 = '''Caption of image: {}\n",
    "This image is taking place in: {}\n",
    "Tags: This image is about {}\n",
    "Describe this image in detail:'''\n",
    "\n",
    "    def get_image_id_from_collection(self, id,collection=GLOBAL_TOKENS_COLLECTION):\n",
    "        results = {}\n",
    "        query = 'FOR doc IN {} FILTER doc.image_id == {} RETURN doc'.format(collection,id)\n",
    "        #print(query)\n",
    "        cursor = self.pipeline.db.aql.execute(query)\n",
    "        for doc in cursor:\n",
    "            results.update(doc)\n",
    "        return results\n",
    "    \n",
    "    def get_structure(self, id):\n",
    "        sg = get_sc_graph(id)\n",
    "        global_doc = self.get_image_id_from_collection(id)\n",
    "        if not global_doc:\n",
    "            print(\"Couldn't find global tokens for id {}\".format(id))\n",
    "            return\n",
    "        rc_doc = {\n",
    "            'image_id': id,\n",
    "            'url': sg.image.url            \n",
    "        }\n",
    "        for (k,v) in global_doc.items():\n",
    "            if k.startswith('global'):\n",
    "                rc_doc[k]=copy.copy(v)\n",
    "        rois = []\n",
    "        for obj in sg.objects:\n",
    "            obj_dic = {\n",
    "                'GT': list(zip(obj.names,[1.0]*len(obj.names)))\n",
    "            }\n",
    "            attr_dic = {\n",
    "                'GT': list(zip(obj.attributes,[1.0]*len(obj.attributes)))\n",
    "            }\n",
    "            obj_doc = {                \n",
    "                'objects': obj.names,\n",
    "                'attributes': obj.attributes,\n",
    "                'bbox': [obj.x, obj.y, obj.x+obj.width, obj.y+obj.height]              \n",
    "                }\n",
    "            rois.append(obj_doc)\n",
    "        rc_doc['rois']=rois\n",
    "\n",
    "        return rc_doc\n",
    "\n",
    "    def get_prompt(self, id, include_answer=False):\n",
    "        base_doc = self.get_structure(id)\n",
    "        if base_doc == None:\n",
    "            return\n",
    "        caption = base_doc['global_captions'][self.global_captioner]\n",
    "        all_objects = sorted(base_doc['global_objects'][self.global_tagger], key=lambda x: -float(x[1]))\n",
    "        all_persons = sorted(base_doc['global_persons'][self.global_tagger], key=lambda x: -float(x[1]))\n",
    "        all_places = sorted(base_doc['global_scenes'][self.places_source], key=lambda x: -float(x[1]))\n",
    "        print(\"Caption: {}\".format(caption))\n",
    "        print(\"Objects: \")\n",
    "        print(all_objects[:5])\n",
    "        print(\"Places:\")\n",
    "        print(all_places[:5])\n",
    "        print(\"Persons:\")\n",
    "        print(all_persons[:5])\n",
    "        objects = '; '.join([x[0] for x in all_objects[:5]])\n",
    "        personds = '; '.join([x[0] for x in all_persons[:5]])\n",
    "        places = ' or '.join([x[0] for x in all_places[:3]])\n",
    "        prompt_before_answer = self.global_prompt1.format(caption,places,objects)\n",
    "        if include_answer:\n",
    "            [answer] = [x['paragraph'] for x in self.ipc_data if x['image_id']==id]\n",
    "            final_prompt = prompt_before_answer+\" \"+answer\n",
    "        else:\n",
    "            final_prompt = prompt_before_answer\n",
    "        return final_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_gen = GTBaseGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a person riding a horse in a field of grass and trees\n",
      "Objects: \n",
      "[['horsewoman', '0.29960856'], ['riding', '0.29438442'], ['horseback', '0.2918341'], ['horseman', '0.29157773'], ['clydesdale', '0.2803259']]\n",
      "Places:\n",
      "[['pasture', '0.2662959'], ['tree farm', '0.22102167'], ['forest path', '0.21471532'], ['vegetation', '0.21412508'], ['racecourse', '0.21341668']]\n",
      "Persons:\n",
      "[['Woman wears a red-and-white', '0.34333515'], ['A red and white jacket', '0.33720917'], ['Red and white jacket', '0.33632693'], ['Woman has on a large', '0.32926083'], ['A red and white', '0.32675546']]\n",
      "Caption of image: a person riding a horse in a field of grass and trees\n",
      "This image is taking place in: pasture or tree farm or forest path\n",
      "Tags: This image is about horsewoman; riding; horseback; horseman; clydesdale\n",
      "Describe this image in detail: In this image a person wearing a red top and black pants is riding a brown and white horse.  The person appears to be young.  The horse is walking slowly with one hoof lifted from the grass.  The rider is holding on to the reins.  The grass is green with yellow straw on top.  There are numerous trees nestled under a blue sky.\n"
     ]
    }
   ],
   "source": [
    "id = 2348389\n",
    "rc = base_gen.get_prompt(id, include_answer=True)\n",
    "print(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cs.stanford.edu/people/rak248/VG_100K/2348389.jpg'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = base_gen.get_structure(id)\n",
    "rc['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gil or dan or moshe'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' or '.join(['gil', 'dan', 'moshe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = vg.get_all_region_descriptions(data_dir=VG_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip([],[1.0]*0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dafab0f5b0f2e0b482ce484a64bf4a63ea947b97362cb54784af04b5754b7b41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
