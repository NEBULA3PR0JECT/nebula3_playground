{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/notebooks/pipenv\")\n",
    "sys.path.insert(0, \"/notebooks/nebula3_database\")\n",
    "sys.path.insert(0, \"/notebooks/nebula3_experiments\")\n",
    "sys.path.insert(0, \"/notebooks/nebula3_videoprocessing\")\n",
    "sys.path.insert(0, \"/notebooks/\")\n",
    "from PIL import Image\n",
    "import requests\n",
    "import visual_genome.local as vg\n",
    "import json\n",
    "import copy\n",
    "import operator\n",
    "import itertools\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import spacy\n",
    "import nltk\n",
    "import openai\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from database.arangodb import DatabaseConnector\n",
    "from config import NEBULA_CONF\n",
    "from vg_eval import VGEvaluation, get_sc_graph, spice_get_triplets, tuples_from_sg\n",
    "from videoprocessing.vlm_factory import VlmFactory\n",
    "from videoprocessing.vlm_interface import VlmInterface\n",
    "from videoprocessing.vlm_implementation import VlmChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# nlp.add_pipe(\"spacy_wordnet\", after='tagger', config={'lang': nlp.lang})\n",
    "\n",
    "with open('/storage/keys/openai.key','r') as f:\n",
    "    OPENAI_API_KEY = f.readline().strip()\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "VG_DATA = '/storage/vg_data'\n",
    "IPC_COLLECTION = 'ipc_relations_spice'\n",
    "RECALL_COLLECTION = 'ipc_recall_spice'\n",
    "GLOBAL_TOKENS_COLLECTION = 's3_global_tokens'\n",
    "S3_PARAGRAPH_1_COLLECTION = 's3_paragraph_1'\n",
    "S3_RECALL_1_COLLECTION = 's3_recall_1'\n",
    "FS_GPT_MODEL = 'text-davinci-002'\n",
    "FS_SAMPLES = 5                   # Samples for few-shot gpt\n",
    "class PIPELINE:\n",
    "    def __init__(self):\n",
    "        config = NEBULA_CONF()\n",
    "        self.db_host = config.get_database_host()\n",
    "        self.database = config.get_playground_name()\n",
    "        self.gdb = DatabaseConnector()\n",
    "        self.db = self.gdb.connect_db(self.database)\n",
    "pipeline = PIPELINE()\n",
    "def get_all_s3_ids():\n",
    "    results = {}\n",
    "    query = 'FOR doc IN {} RETURN doc.image_id'.format(GLOBAL_TOKENS_COLLECTION)\n",
    "    cursor = pipeline.db.aql.execute(query)\n",
    "    return [doc for doc in cursor]\n",
    "\n",
    "s3_ids = get_all_s3_ids()\n",
    "evaluator = VGEvaluation()\n",
    "def flatten(lst): return [x for l in lst for x in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id_from_collection(id,collection=GLOBAL_TOKENS_COLLECTION):\n",
    "    results = {}\n",
    "    query = 'FOR doc IN {} FILTER doc.image_id == {} RETURN doc'.format(collection,id)\n",
    "    cursor = pipeline.db.aql.execute(query)\n",
    "    for doc in cursor:\n",
    "        results.update(doc)\n",
    "    return results\n",
    "\n",
    "class GTBaseGenerator:\n",
    "    def __init__(self):\n",
    "        self.pipeline = PIPELINE()\n",
    "        self.ipc_data = json.load(open('/storage/ipc_data/paragraphs_v1.json','r'))\n",
    "        self.global_captioner = 'blip'\n",
    "        self.global_tagger = 'blip'\n",
    "        self.places_source = 'blip'\n",
    "        self.global_prompt1 = '''Caption of image: {}\n",
    "This image is taking place in: {}\n",
    "Tags: This image is about {}\n",
    "Describe this image in detail:'''\n",
    "\n",
    "    \n",
    "    def get_structure(self, id):\n",
    "        sg = get_sc_graph(id)\n",
    "        global_doc = get_image_id_from_collection(id)\n",
    "        if not global_doc:\n",
    "            print(\"Couldn't find global tokens for id {}\".format(id))\n",
    "            return\n",
    "        rc_doc = {\n",
    "            'image_id': id,\n",
    "            'url': sg.image.url            \n",
    "        }\n",
    "        for (k,v) in global_doc.items():\n",
    "            if k.startswith('global'):\n",
    "                rc_doc[k]=copy.copy(v)\n",
    "        rois = []\n",
    "        for obj in sg.objects:\n",
    "            obj_dic = {\n",
    "                'GT': list(zip(obj.names,[1.0]*len(obj.names)))\n",
    "            }\n",
    "            attr_dic = {\n",
    "                'GT': list(zip(obj.attributes,[1.0]*len(obj.attributes)))\n",
    "            }\n",
    "            obj_doc = {                \n",
    "                'objects': obj.names,\n",
    "                'attributes': obj.attributes,\n",
    "                'bbox': [obj.x, obj.y, obj.x+obj.width, obj.y+obj.height]              \n",
    "                }\n",
    "            rois.append(obj_doc)\n",
    "        rc_doc['rois']=rois\n",
    "\n",
    "        return rc_doc\n",
    "\n",
    "    def get_prompt(self, id, include_answer=False):\n",
    "        base_doc = self.get_structure(id)\n",
    "        if base_doc == None:\n",
    "            return\n",
    "        caption = base_doc['global_captions'][self.global_captioner]\n",
    "        all_objects = base_doc['global_objects'][self.global_tagger]\n",
    "        all_persons = base_doc['global_persons'][self.global_tagger]\n",
    "        all_places = base_doc['global_scenes'][self.places_source]\n",
    "        # print(\"Caption: {}\".format(caption))\n",
    "        # print(\"Objects: \")\n",
    "        # print(all_objects[:5])\n",
    "        # print(\"Places:\")\n",
    "        # print(all_places[:5])\n",
    "        # print(\"Persons:\")\n",
    "        # print(all_persons[:5])\n",
    "        objects = '; '.join([x['label'] for x in all_objects[:8]])\n",
    "        persons = '; '.join([x['label'] for x in all_persons[:5]])\n",
    "        places = ' or '.join([x['label'] for x in all_places[:3]])\n",
    "        prompt_before_answer = self.global_prompt1.format(caption,places,objects)\n",
    "        if include_answer:\n",
    "            [answer] = [x['paragraph'] for x in self.ipc_data if x['image_id']==id]\n",
    "            final_prompt = prompt_before_answer+\" \"+answer\n",
    "        else:\n",
    "            final_prompt = prompt_before_answer\n",
    "        return final_prompt\n",
    "        \n",
    "base_gen = GTBaseGenerator()\n",
    "\n",
    "def few_shot_process_target_id(fs_ids: list[int],target_id: int, vlm: VlmInterface, pgen=base_gen, **kwargs):\n",
    "    target_sg = get_sc_graph(target_id)\n",
    "    fs_prompt = generate_gpt_prompt(fs_ids, target_id=target_id, pgen=pgen)\n",
    "    results = gpt_execute(fs_prompt, model=FS_GPT_MODEL, **kwargs)\n",
    "    return results\n",
    "    # scores = vlm.compute_similarity_url(target_sg.image.url,results)\n",
    "    # best_index = np.argmax(scores)\n",
    "    # return results[best_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_execute(prompt_template, *args, **kwargs):            \n",
    "    prompt = prompt_template.format(*args)   \n",
    "    response = openai.Completion.create(prompt=prompt, max_tokens=256, **kwargs)   \n",
    "    # return response\n",
    "    return [x['text'].strip() for x in response['choices']]\n",
    "\n",
    "def generate_gpt_prompt(ids, target_id=None, pgen=GTBaseGenerator()):\n",
    "    rc = []\n",
    "    for id in ids:\n",
    "        rc.append(pgen.get_prompt(id,include_answer=True))\n",
    "    if target_id:\n",
    "        rc.append(pgen.get_prompt(target_id,include_answer=False))\n",
    "    return '\\n'.join(rc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_from_paragraph(paragraph):\n",
    "    senter = nlp.get_pipe(\"senter\")\n",
    "    sentences = [str(x) for x in senter(nlp(paragraph)).sents]\n",
    "    n = len(sentences)\n",
    "    cands = []\n",
    "    for i in range(3,n+1):\n",
    "        for comb in itertools.combinations(range(n),i):\n",
    "            cands.append(' '.join(operator.itemgetter(*comb)(sentences)))\n",
    "    return cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model on GPU\n",
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth\n"
     ]
    }
   ],
   "source": [
    "vlm = VlmChunker(VlmFactory().get_vlm(\"blip_itc\"), chunk_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train, s3_test = np.split(np.array(s3_ids),[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ids = np.random.choice(s3_train,5)\n",
    "# target_id = np.random.choice(s3_test)\n",
    "# # rc = generate_gpt_prompt(train_ids, target_id=target_id, pgen=base_gen)\n",
    "# rc = few_shot_process_target_id(train_ids,target_id,vlm,n=5)\n",
    "# candidates = flatten([candidates_from_paragraph(x) for x in rc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_target_id(target_id, train_ids, vlm, n=5, **kwargs):\n",
    "    sg = get_sc_graph(target_id)\n",
    "    print(\"Processing target_id {}, url: {}\".format(target_id,sg.image.url))\n",
    "    train_ids = np.random.choice(s3_train,FS_SAMPLES)\n",
    "    rc = few_shot_process_target_id(train_ids, target_id, vlm, n=n, **kwargs)\n",
    "    candidates = flatten([candidates_from_paragraph(x) for x in rc])\n",
    "    scores = vlm.compute_similarity_url(sg.image.url,candidates)\n",
    "    cand = candidates[np.argmax(scores)]\n",
    "    return {\n",
    "        'image_id': target_id,\n",
    "        'url': sg.image.url,\n",
    "        'paragraphs': rc,\n",
    "        'candidate': cand\n",
    "    }\n",
    "\n",
    "def process_recall(target_id):\n",
    "    doc = get_image_id_from_collection(target_id,S3_PARAGRAPH_1_COLLECTION)\n",
    "    if not doc:\n",
    "        print(\"Paragraph document for target id {} missing. Aborting\".format(target_id))\n",
    "        return\n",
    "    sg = get_sc_graph(target_id)\n",
    "    gt_triplets = tuples_from_sg(sg)\n",
    "    pred_triplets = spice_get_triplets(doc['candidate'])\n",
    "    # print(\"Ground Triplets:\")\n",
    "    # print(gt_triplets)\n",
    "    # print(\"Pred triplets:\")\n",
    "    # print(pred_triplets)\n",
    "    recall = evaluator.recall_triplets_mean(gt_triplets,pred_triplets)\n",
    "    precision = evaluator.recall_triplets_mean(pred_triplets,gt_triplets)\n",
    "    return {\n",
    "        'image_id': target_id,\n",
    "        'mean_recall': recall,\n",
    "        'mean_precision': precision\n",
    "    }\n",
    "\n",
    "\n",
    "def process_gen_paragraph_all_ids(train_ids, target_ids, vlm, **kwargs):\n",
    "    for target_id in target_ids:\n",
    "        rc = get_image_id_from_collection(target_id,S3_PARAGRAPH_1_COLLECTION)\n",
    "        if rc:\n",
    "            print(\"target id {} already exists.\".format(target_id))\n",
    "            continue\n",
    "\n",
    "        print(\"Moving on with target id {}.\".format(target_id))\n",
    "        rc_doc = process_target_id(target_id, train_ids, vlm, **kwargs)\n",
    "        query = \"INSERT {} INTO {}\".format(rc_doc,S3_PARAGRAPH_1_COLLECTION)\n",
    "        cursor = pipeline.db.aql.execute(query)  \n",
    "\n",
    "def process_recall_all_ids(target_ids):\n",
    "    for target_id in target_ids:\n",
    "        rc = get_image_id_from_collection(target_id,S3_RECALL_1_COLLECTION)\n",
    "        if rc:\n",
    "            print(\"target id {} already exists.\".format(target_id))\n",
    "            continue\n",
    "\n",
    "        print(\"Moving on with target id {}.\".format(target_id))\n",
    "        rc_doc = process_recall(target_id)\n",
    "        query = \"INSERT {} INTO {}\".format(rc_doc,S3_RECALL_1_COLLECTION)\n",
    "        cursor = pipeline.db.aql.execute(query)  \n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_gen_paragraph_all_ids(s3_train,s3_test,vlm,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target id 2370676 already exists.\n",
      "target id 2402584 already exists.\n",
      "target id 2390591 already exists.\n",
      "Moving on with target id 2334467.\n",
      "Moving on with target id 2405264.\n",
      "Moving on with target id 2345862.\n",
      "Moving on with target id 2322127.\n",
      "Moving on with target id 2345047.\n",
      "Moving on with target id 2400355.\n",
      "Moving on with target id 4550.\n",
      "Moving on with target id 2366505.\n",
      "Moving on with target id 2340989.\n",
      "Moving on with target id 2412473.\n",
      "Moving on with target id 2407398.\n",
      "Moving on with target id 2396279.\n",
      "Moving on with target id 2343486.\n",
      "Moving on with target id 2348753.\n",
      "Moving on with target id 2356333.\n",
      "Moving on with target id 2342746.\n",
      "Moving on with target id 2394355.\n",
      "Moving on with target id 2371761.\n",
      "Moving on with target id 2412770.\n",
      "Moving on with target id 2388946.\n",
      "Moving on with target id 2373834.\n",
      "Moving on with target id 2360872.\n",
      "Moving on with target id 2401158.\n",
      "Moving on with target id 2263.\n",
      "Moving on with target id 2372687.\n",
      "Moving on with target id 2322096.\n",
      "Moving on with target id 2316590.\n",
      "Moving on with target id 2356940.\n",
      "Moving on with target id 2353864.\n",
      "Moving on with target id 2363469.\n",
      "Moving on with target id 2380104.\n",
      "Moving on with target id 2389534.\n",
      "Moving on with target id 2384225.\n",
      "Moving on with target id 2330715.\n",
      "Moving on with target id 2400675.\n",
      "Moving on with target id 2357443.\n",
      "Moving on with target id 2334939.\n",
      "Moving on with target id 2404369.\n",
      "Moving on with target id 2346178.\n",
      "Moving on with target id 2417901.\n",
      "Moving on with target id 2413206.\n",
      "Moving on with target id 2359778.\n",
      "Moving on with target id 2378674.\n",
      "Moving on with target id 2353984.\n",
      "Moving on with target id 2405430.\n",
      "Moving on with target id 2380874.\n",
      "Moving on with target id 2362882.\n",
      "Moving on with target id 2378353.\n",
      "Moving on with target id 2414780.\n",
      "Moving on with target id 2333956.\n",
      "Moving on with target id 2402383.\n",
      "Moving on with target id 2409065.\n",
      "Moving on with target id 2376219.\n",
      "Moving on with target id 2384133.\n",
      "Moving on with target id 2374836.\n",
      "Moving on with target id 2390759.\n",
      "Moving on with target id 2404544.\n",
      "Moving on with target id 2369494.\n",
      "Moving on with target id 2339221.\n",
      "Moving on with target id 2413006.\n",
      "Moving on with target id 2375861.\n",
      "Moving on with target id 2372079.\n",
      "Moving on with target id 2384690.\n",
      "Moving on with target id 2344455.\n",
      "Moving on with target id 2325585.\n",
      "Moving on with target id 2394596.\n",
      "Moving on with target id 2348910.\n",
      "Moving on with target id 2407256.\n",
      "Moving on with target id 2375362.\n",
      "Moving on with target id 2377072.\n",
      "Moving on with target id 2365546.\n",
      "Moving on with target id 2366591.\n",
      "Moving on with target id 2393746.\n",
      "Moving on with target id 2329637.\n",
      "Moving on with target id 2412528.\n",
      "Moving on with target id 2395833.\n",
      "Moving on with target id 2348812.\n",
      "Moving on with target id 2333219.\n",
      "Moving on with target id 2388484.\n",
      "Moving on with target id 2375502.\n",
      "Moving on with target id 2415125.\n",
      "Moving on with target id 2368665.\n",
      "Moving on with target id 2367831.\n",
      "Moving on with target id 2350454.\n",
      "Moving on with target id 2342570.\n",
      "Moving on with target id 2404031.\n",
      "Moving on with target id 2332030.\n",
      "Moving on with target id 2351592.\n",
      "Moving on with target id 2364100.\n",
      "Moving on with target id 2396271.\n",
      "Moving on with target id 2359398.\n",
      "Moving on with target id 2369148.\n",
      "Moving on with target id 2403903.\n",
      "Moving on with target id 2370646.\n",
      "Moving on with target id 2413315.\n",
      "Moving on with target id 2396852.\n",
      "Moving on with target id 2372024.\n",
      "Moving on with target id 2358255.\n",
      "Moving on with target id 2402212.\n",
      "Moving on with target id 2381356.\n",
      "Moving on with target id 2404550.\n",
      "Moving on with target id 2382233.\n",
      "Moving on with target id 2339495.\n",
      "Moving on with target id 2395158.\n",
      "Moving on with target id 2374232.\n",
      "Moving on with target id 2379764.\n",
      "Moving on with target id 2361208.\n",
      "Moving on with target id 2412167.\n",
      "Moving on with target id 2323358.\n",
      "Moving on with target id 2325024.\n",
      "Moving on with target id 2359616.\n",
      "Moving on with target id 2323344.\n",
      "Moving on with target id 2367729.\n",
      "Moving on with target id 2370282.\n",
      "Moving on with target id 2387230.\n",
      "Moving on with target id 2370342.\n",
      "Moving on with target id 2386473.\n",
      "Moving on with target id 2365683.\n",
      "Moving on with target id 2415236.\n",
      "Moving on with target id 2383741.\n",
      "Moving on with target id 2408361.\n",
      "Moving on with target id 2371466.\n",
      "Moving on with target id 2326518.\n",
      "Moving on with target id 2368674.\n",
      "Moving on with target id 2377813.\n",
      "Moving on with target id 2411690.\n",
      "Moving on with target id 2357874.\n",
      "Moving on with target id 2366712.\n",
      "Moving on with target id 2346862.\n",
      "Moving on with target id 2380610.\n",
      "Moving on with target id 2411245.\n",
      "Moving on with target id 2342466.\n",
      "Moving on with target id 2360809.\n",
      "Moving on with target id 2323611.\n",
      "Moving on with target id 2390170.\n",
      "Moving on with target id 2385486.\n",
      "Moving on with target id 2381288.\n",
      "Moving on with target id 2316166.\n",
      "Moving on with target id 2356436.\n",
      "Moving on with target id 2332941.\n",
      "Moving on with target id 2367471.\n",
      "Moving on with target id 2392137.\n",
      "Moving on with target id 2350124.\n",
      "Moving on with target id 2408608.\n",
      "Moving on with target id 2402287.\n",
      "Moving on with target id 2396963.\n",
      "Moving on with target id 2330793.\n",
      "Moving on with target id 2353479.\n",
      "Moving on with target id 2377885.\n",
      "Moving on with target id 2416169.\n",
      "Moving on with target id 2324748.\n",
      "Moving on with target id 2373757.\n",
      "Moving on with target id 2393701.\n",
      "Moving on with target id 2408865.\n",
      "Moving on with target id 2316381.\n",
      "Moving on with target id 2344255.\n",
      "Moving on with target id 2370808.\n",
      "Moving on with target id 2374159.\n",
      "Moving on with target id 2381031.\n",
      "Moving on with target id 2323875.\n",
      "Moving on with target id 2359964.\n",
      "Moving on with target id 2401996.\n",
      "Moving on with target id 2347435.\n",
      "Moving on with target id 2407936.\n",
      "Moving on with target id 1231.\n",
      "Moving on with target id 2348025.\n",
      "Moving on with target id 2380568.\n",
      "Moving on with target id 2404490.\n",
      "Moving on with target id 2379310.\n",
      "Moving on with target id 2409189.\n",
      "Moving on with target id 713283.\n",
      "Moving on with target id 2391002.\n",
      "Moving on with target id 285706.\n",
      "Moving on with target id 2346162.\n",
      "Moving on with target id 2375091.\n",
      "Moving on with target id 2386268.\n",
      "Moving on with target id 2375975.\n",
      "Moving on with target id 2333244.\n",
      "Moving on with target id 2370679.\n",
      "Moving on with target id 2335662.\n",
      "Moving on with target id 2410955.\n",
      "Moving on with target id 2388846.\n",
      "Moving on with target id 2371836.\n",
      "Moving on with target id 2415895.\n",
      "Moving on with target id 2361833.\n",
      "Moving on with target id 2385479.\n",
      "Moving on with target id 2379307.\n",
      "Moving on with target id 2406992.\n",
      "Moving on with target id 2365188.\n",
      "Moving on with target id 2347337.\n",
      "Moving on with target id 2374704.\n",
      "Moving on with target id 2364458.\n",
      "Moving on with target id 2395945.\n",
      "Moving on with target id 2366911.\n",
      "Moving on with target id 2318780.\n",
      "Moving on with target id 2316661.\n",
      "Moving on with target id 2359808.\n"
     ]
    }
   ],
   "source": [
    "process_recall_all_ids(s3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 2402584, 'mean_recall': 0.5702875, 'mean_precision': 0.68995017}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = get_sc_graph(target_id)\n",
    "print(sg.image.url)\n",
    "gt_triplets = tuples_from_sg(sg)\n",
    "pred_triplets = spice_get_triplets(gpt_rc[0])\n",
    "recall = evaluator.recall_triplets_mean(gt_triplets,pred_triplets)\n",
    "print(\"Mean (bert-based) total recall of ground truth triplets in ipc triplets is: {}\".format(recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = get_sc_graph(2359808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x.x, x.y, x.x+x.width, x.y+x.height) for x in sg.objects if x.width>=50 and x.height>=50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = senter(nlp(\"Hello world. Hello world again.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(rc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dafab0f5b0f2e0b482ce484a64bf4a63ea947b97362cb54784af04b5754b7b41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
