{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/notebooks/pipenv\")\n",
    "sys.path.insert(0, \"/notebooks/nebula3_vlm\")\n",
    "sys.path.insert(0, \"/notebooks/nebula3_database\")\n",
    "sys.path.insert(0, \"/notebooks/\")\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import bisect\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib\n",
    "import subprocess\n",
    "import re\n",
    "import tempfile\n",
    "import itertools\n",
    "import torch\n",
    "import spacy\n",
    "# import amrlib\n",
    "# import penman\n",
    "\n",
    "from typing import List, Tuple\n",
    "from operator import itemgetter \n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "from database.arangodb import DatabaseConnector\n",
    "from config import NEBULA_CONF\n",
    "from movie_db import MOVIE_DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIPELINE:\n",
    "    def __init__(self):\n",
    "        config = NEBULA_CONF()\n",
    "        self.db_host = config.get_database_host()\n",
    "        self.database = config.get_playground_name()\n",
    "        self.gdb = DatabaseConnector()\n",
    "        self.db = self.gdb.connect_db(self.database)\n",
    "\n",
    "pipeline = PIPELINE()\n",
    "mdb = MOVIE_DB()\n",
    "from vlm.clip_api import CLIP_API\n",
    "clip=CLIP_API('vit')\n",
    "s2_collection_name = 's2_pipeline_after_gpt'\n",
    "s2_results_orig_collection_name = 's2_pipeline_optim_orig'\n",
    "s2_results_relaxed_collection_name = 's2_pipeline_optim_relaxed'\n",
    "s2_compatibility_collection_name = 's2_pipeline_compatibility_scores'\n",
    "s2_with_compat_collection_name = 's2_pipeline_compatibility_results'\n",
    "s2_with_compat_collection_name2 = 's2_pipeline_compatibility_results2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst): return [x for l in lst for x in l]\n",
    "\n",
    "def compute_batch_scores(video_emb: torch.Tensor, texts: List[str], normalize=True, **kwargs) -> List[float]:    \n",
    "    emb_batch = clip.clip_batch_encode_text(texts, **kwargs)                           \n",
    "    return (video_emb.expand_as(emb_batch)*emb_batch).sum(dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "def compute_concat_score(image_emb: torch.Tensor, texts: List[str], join_on=',') -> float:\n",
    "    combined_text = \"\"\n",
    "    for t in [x.strip() for x in texts]:\n",
    "        if t[-1]=='.':\n",
    "            t = t[:-1]       \n",
    "        t+=join_on\n",
    "        t+=' '\n",
    "        combined_text+=t\n",
    "    print(\"Combined: \"+combined_text)\n",
    "    return torch.matmul(image_emb,mdmmt.encode_text(combined_text.strip()) )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityManager:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "    def similarity(self, src, target):\n",
    "        rc = []\n",
    "        s1 = self.nlp(src)\n",
    "        s2 = self.nlp(target)\n",
    "        for w in s1:\n",
    "            # if not w or not w.vector_norm:\n",
    "            #     print('Argghhh 1, bad word:')\n",
    "            #     print(w.text)\n",
    "            if w.pos_ not in ['NOUN', 'ADJ', 'ADV', 'VERB', 'PROPN', 'ADP'] and len(s1)>1:\n",
    "                continue\n",
    "            # for tok in s2:\n",
    "            #     if not tok or not tok.vector_norm:\n",
    "            #         print('Argghhh 2, bad word:')\n",
    "            #         print(tok.text)\n",
    "            #         print(s2.text)\n",
    "            rc.append(max([w.similarity(x) for x in s2]))\n",
    "        return np.mean(rc)\n",
    "        \n",
    "smanager = SimilarityManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = lambda x: np.exp(x)/sum(np.exp(x))\n",
    "def normalize(x):\n",
    "    epsilon = 0.00001\n",
    "    if np.std(x) < epsilon:\n",
    "        return np.ones(x.shape)\n",
    "    return (x - np.mean(x)) / np.std(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(1,len(s)+1))\n",
    "\n",
    "\n",
    "def optimize_sents(emb_video, experts, sents, compat_scores, use_ordered_scores=False):\n",
    "    smanager = SimilarityManager()\n",
    "    compat_scores = np.array(compat_scores)\n",
    "    as_compat = compat_scores.argsort()\n",
    "    # print(compat_scores)\n",
    "    graded_scores = sorted(list(zip(as_compat,range(len(as_compat)))),key = lambda x:x[0])\n",
    "    # print(list(zip(as_compat,range(len(as_compat)))))\n",
    "    # print(graded_scores)\n",
    "    order_scores = normalize(np.array(list(zip(*graded_scores))[1]))\n",
    "    print(order_scores)\n",
    "    # print(list(zip(compat_scores[as_compat],as_compat)))\n",
    "    orig_similarity = compute_batch_scores(emb_video, sents)\n",
    "    candidates_similarity = normalize(orig_similarity)\n",
    "    coverage_matrix = np.zeros([len(experts),len(sents)])\n",
    "    coverage_matrix[:] = np.nan\n",
    "    for i in range(len(experts)):\n",
    "        for j in range(len(sents)):\n",
    "            coverage_matrix[i][j]=smanager.similarity(experts[i],sents[j])\n",
    "        coverage_matrix[i] = normalize(coverage_matrix[i])\n",
    "\n",
    "    def get_score(state: List[int]) -> float:\n",
    "        theta_similarity = 1.\n",
    "        theta_coverage = 1.3\n",
    "        theta_compat = 1.\n",
    "        if not state:\n",
    "            return 0\n",
    "        coverage_score = get_state_coverage(state)   \n",
    "        similarity_score = candidates_similarity[state].mean().item()\n",
    "        if use_ordered_scores:            \n",
    "             compat_score = order_scores[state].mean().item()\n",
    "        else:            \n",
    "             compat_score = compat_scores[state].mean().item()\n",
    "        return theta_coverage*coverage_score + theta_similarity*similarity_score + theta_compat*compat_score\n",
    "\n",
    "    def get_expert_coverage(state):\n",
    "        # return self.coverage_matrix[:,state].sum(axis=1)\n",
    "        return coverage_matrix[:,state].max(axis=1)\n",
    "          \n",
    "    def get_state_coverage(state) -> float:\n",
    "        # print(\"State coverage for {}:\".format(state))\n",
    "        # print(get_expert_coverage(state))\n",
    "        return np.mean(get_expert_coverage(state))\n",
    "\n",
    "\n",
    "    superset = list(range(len(sents)))\n",
    "    pset = [list(x) for x in powerset(superset)]\n",
    "    pset_scores = [get_score(x) for x in pset]\n",
    "    best_cand = pset[np.argmax(pset_scores)]\n",
    "    print(\"Best candidates:\")\n",
    "    print(best_cand)\n",
    "    rc_sents = itemgetter(*best_cand)(sents)\n",
    "    if type(rc_sents)==tuple:\n",
    "        rc = list(rc_sents)\n",
    "    elif type(rc_sents)==str:\n",
    "        rc = [rc_sents]\n",
    "    else:\n",
    "        print(\"Bad return type!!\")\n",
    "    return rc, orig_similarity[best_cand].mean()\n",
    "\n",
    "\n",
    "def optimize_scene(doc,mat=None, emb_video=None, **kwargs):\n",
    "    mid = doc['movie_id']\n",
    "    elem = doc['scene_element']\n",
    "    emb_video = clip.clip_encode_video(mid,elem)\n",
    "    all_sents = doc['sentences']\n",
    "    rc = mdb.get_scene_from_collection(mid,elem,'s2_clsmdc')    \n",
    "    experts = flatten(rc['experts'].values())\n",
    "    rc = mdb.get_scene_from_collection(mid,elem,s2_compatibility_collection_name)  \n",
    "    all_compat_scores = rc['compat_scores']\n",
    "    n = len(all_sents)\n",
    "    rc_sents = n*[None]\n",
    "    mean_scores = n*[None]\n",
    "    for i in range(n):\n",
    "        rc_sents[i], mean_scores[i] = optimize_sents(emb_video,experts,all_sents[i],all_compat_scores[i], **kwargs)\n",
    "\n",
    "    return rc_sents, mean_scores\n",
    "    \n",
    "def run_pipeline(all_docs, target_collection_name=s2_with_compat_collection_name, **kwargs):\n",
    "    for doc in all_docs:\n",
    "        mid = doc['movie_id']\n",
    "        elem = doc['scene_element']\n",
    "        rc = mdb.get_scene_from_collection(mid,elem,target_collection_name)\n",
    "        if rc:\n",
    "            print(\"Results already exist for {}/{}\".format(mid,elem))\n",
    "            continue\n",
    "        print(\"Going forward with {}/{}\".format(mid,elem))\n",
    "\n",
    "        rc_sents, sim_scores = optimize_scene(doc,**kwargs)\n",
    "        rc_doc = {\n",
    "            'movie_id': mid,\n",
    "            'scene_element': elem,\n",
    "            'sentences': rc_sents,\n",
    "            'mean_scores': sim_scores,\n",
    "        }\n",
    "        query = \"INSERT {} INTO {}\".format(rc_doc,target_collection_name)\n",
    "        cursor = pipeline.db.aql.execute(query)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid is Movies/222509871\n",
      "mid is Movies/222509634\n",
      "mid is Movies/222510253\n",
      "mid is Movies/222509820\n",
      "mid is Movies/222511030\n",
      "mid is Movies/222510046\n",
      "mid is Movies/222509945\n",
      "mid is Movies/222510324\n",
      "mid is Movies/222510189\n",
      "mid is Movies/222510575\n",
      "mid is Movies/222510810\n",
      "mid is Movies/222509721\n",
      "mid is Movies/222510136\n",
      "mid is Movies/222510448\n",
      "mid is Movies/222511095\n",
      "mid is Movies/222510951\n",
      "mid is Movies/222510403\n",
      "mid is Movies/222510692\n"
     ]
    }
   ],
   "source": [
    "query = 'FOR doc IN {} RETURN doc'.format(s2_compatibility_collection_name)\n",
    "cursor = pipeline.db.aql.execute(query)\n",
    "all_docs = sorted(list(cursor), key=lambda x:\"{}/{}\".format(x['movie_id'],x['scene_element']))\n",
    "movies = list(set([x['movie_id'] for x in all_docs]))\n",
    "all_movies = {}\n",
    "\n",
    "for mid in movies:\n",
    "    print(\"mid is {}\".format(mid))\n",
    "    story = []\n",
    "    elements = sorted([x for x in all_docs if x['movie_id'] == mid],key=lambda y:y['scene_element'])\n",
    "    all_movies[mid] = elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(doc, num_sigma=5.0, num_abs=0.07, use_abs=True):\n",
    "    n = len(doc['compat_scores'])\n",
    "    rc = []\n",
    "    for i in range(n):\n",
    "        scores = np.array(doc['compat_scores'][i])\n",
    "        mean = scores.mean()\n",
    "        var = scores.std()\n",
    "        if use_abs:\n",
    "            rc.append(np.where(np.abs(scores-mean)>num_abs)[0])\n",
    "        else:\n",
    "            rc.append(np.where(np.abs(scores-mean)>num_sigma*var)[0])\n",
    "\n",
    "    return rc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outliers = []\n",
    "for doc in all_docs:    \n",
    "    rc = detect_outliers(doc,num_abs=0.07,use_abs=True)\n",
    "    if len(flatten(rc)) > 0:\n",
    "        all_outliers.append((doc['movie_id'],doc['scene_element']))\n",
    "\n",
    "len(all_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Movies/222510575', 10),\n",
       " ('Movies/222510575', 9),\n",
       " ('Movies/222510692', 9),\n",
       " ('Movies/222510951', 2)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([], dtype=int64)),\n",
       " (1, array([], dtype=int64)),\n",
       " (2, array([], dtype=int64)),\n",
       " (3, array([], dtype=int64)),\n",
       " (4, array([], dtype=int64)),\n",
       " (5, array([], dtype=int64)),\n",
       " (6, array([], dtype=int64)),\n",
       " (7, array([], dtype=int64)),\n",
       " (8, array([], dtype=int64)),\n",
       " (9, array([3]))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid, elem = all_outliers[1]\n",
    "doc = all_movies[mid][elem]\n",
    "emb_video = clip.clip_encode_video(mid,elem)\n",
    "list(enumerate(detect_outliers(doc,use_abs=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ('a group of missiles is going away over a green mountain and a trail of smoke is coming from probably a burnt target',\n",
       "   0.09735290688455547)),\n",
       " (1, ('a group of missiles are flying over the sea', 0.06099481262749507)),\n",
       " (2,\n",
       "  ('a group of planes flying towards a green mountain and targeting a beach behind it',\n",
       "   0.10965346281374284)),\n",
       " (3,\n",
       "  ('group of missiles flying over a green mountain targeting the beach',\n",
       "   0.012350157983723417)),\n",
       " (4, ('a group of planes flying over a beach', 0.12592096007619308)),\n",
       " (5,\n",
       "  ('a group of missiles are approaching a beach from a body of green mountains',\n",
       "   0.06134737122207453)),\n",
       " (6,\n",
       "  ('a group of planes flying over the sea and targeting a beach',\n",
       "   0.12574007790496516)),\n",
       " (7,\n",
       "  ('a group of planes flying over a green mountain with a trail of smoke nearby',\n",
       "   0.11448714745906782)),\n",
       " (8,\n",
       "  ('a group of planes flying over a body of water and targeting the beach',\n",
       "   0.11872658162289078)),\n",
       " (9,\n",
       "  ('a group of missiles flying over a body of water with a trail of smoke targeting the beach',\n",
       "   0.09733115069724876)),\n",
       " (10, ('a group of missiles flying over a beach', 0.07609537070804302))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=9\n",
    "compute_batch_scores\n",
    "list(enumerate(zip(doc['sentences'][i],doc['compat_scores'][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dafab0f5b0f2e0b482ce484a64bf4a63ea947b97362cb54784af04b5754b7b41"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
