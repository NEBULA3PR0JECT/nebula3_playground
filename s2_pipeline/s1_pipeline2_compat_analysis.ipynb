{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/notebooks/pipenv\")\n",
    "sys.path.insert(0, \"/notebooks/nebula3_vlm\")\n",
    "sys.path.insert(0, \"/notebooks/nebula3_database\")\n",
    "sys.path.insert(0, \"/notebooks/\")\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import bisect\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib\n",
    "import subprocess\n",
    "import re\n",
    "import tempfile\n",
    "import itertools\n",
    "import torch\n",
    "import spacy\n",
    "# import amrlib\n",
    "# import penman\n",
    "\n",
    "from typing import List, Tuple\n",
    "from operator import itemgetter \n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "from database.arangodb import DatabaseConnector\n",
    "from config import NEBULA_CONF\n",
    "from movie_db import MOVIE_DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIPELINE:\n",
    "    def __init__(self):\n",
    "        config = NEBULA_CONF()\n",
    "        self.db_host = config.get_database_host()\n",
    "        self.database = config.get_playground_name()\n",
    "        self.gdb = DatabaseConnector()\n",
    "        self.db = self.gdb.connect_db(self.database)\n",
    "\n",
    "pipeline = PIPELINE()\n",
    "mdb = MOVIE_DB()\n",
    "from vlm.clip_api import CLIP_API\n",
    "clip=CLIP_API('vit')\n",
    "s2_collection_name = 's2_pipeline_after_gpt'\n",
    "s2_results_orig_collection_name = 's2_pipeline_optim_orig'\n",
    "s2_results_relaxed_collection_name = 's2_pipeline_optim_relaxed'\n",
    "s2_compatibility_collection_name = 's2_pipeline_compatibility_scores'\n",
    "s2_with_compat_collection_name = 's2_pipeline_compatibility_results'\n",
    "s2_with_compat_collection_name2 = 's2_pipeline_compatibility_results2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst): return [x for l in lst for x in l]\n",
    "\n",
    "def compute_batch_scores(video_emb: torch.Tensor, texts: List[str], normalize=True, **kwargs) -> List[float]:    \n",
    "    emb_batch = clip.clip_batch_encode_text(texts, **kwargs)                           \n",
    "    return (video_emb.expand_as(emb_batch)*emb_batch).sum(dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "def compute_concat_score(image_emb: torch.Tensor, texts: List[str], join_on=',') -> float:\n",
    "    combined_text = \"\"\n",
    "    for t in [x.strip() for x in texts]:\n",
    "        if t[-1]=='.':\n",
    "            t = t[:-1]       \n",
    "        t+=join_on\n",
    "        t+=' '\n",
    "        combined_text+=t\n",
    "    print(\"Combined: \"+combined_text)\n",
    "    return torch.matmul(image_emb,mdmmt.encode_text(combined_text.strip()) )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityManager:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "    def similarity(self, src, target):\n",
    "        rc = []\n",
    "        s1 = self.nlp(src)\n",
    "        s2 = self.nlp(target)\n",
    "        for w in s1:\n",
    "            # if not w or not w.vector_norm:\n",
    "            #     print('Argghhh 1, bad word:')\n",
    "            #     print(w.text)\n",
    "            if w.pos_ not in ['NOUN', 'ADJ', 'ADV', 'VERB', 'PROPN', 'ADP'] and len(s1)>1:\n",
    "                continue\n",
    "            # for tok in s2:\n",
    "            #     if not tok or not tok.vector_norm:\n",
    "            #         print('Argghhh 2, bad word:')\n",
    "            #         print(tok.text)\n",
    "            #         print(s2.text)\n",
    "            rc.append(max([w.similarity(x) for x in s2]))\n",
    "        return np.mean(rc)\n",
    "        \n",
    "smanager = SimilarityManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = lambda x: np.exp(x)/sum(np.exp(x))\n",
    "def normalize(x):\n",
    "    epsilon = 0.00001\n",
    "    if np.std(x) < epsilon:\n",
    "        return np.ones(x.shape)\n",
    "    return (x - np.mean(x)) / np.std(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(1,len(s)+1))\n",
    "\n",
    "\n",
    "def optimize_sents(emb_video, experts, sents, compat_scores, use_ordered_scores=False):\n",
    "    smanager = SimilarityManager()\n",
    "    compat_scores = np.array(compat_scores)\n",
    "    as_compat = compat_scores.argsort()\n",
    "    # print(compat_scores)\n",
    "    graded_scores = sorted(list(zip(as_compat,range(len(as_compat)))),key = lambda x:x[0])\n",
    "    # print(list(zip(as_compat,range(len(as_compat)))))\n",
    "    # print(graded_scores)\n",
    "    order_scores = normalize(np.array(list(zip(*graded_scores))[1]))\n",
    "    print(order_scores)\n",
    "    # print(list(zip(compat_scores[as_compat],as_compat)))\n",
    "    orig_similarity = compute_batch_scores(emb_video, sents)\n",
    "    candidates_similarity = normalize(orig_similarity)\n",
    "    coverage_matrix = np.zeros([len(experts),len(sents)])\n",
    "    coverage_matrix[:] = np.nan\n",
    "    for i in range(len(experts)):\n",
    "        for j in range(len(sents)):\n",
    "            coverage_matrix[i][j]=smanager.similarity(experts[i],sents[j])\n",
    "        coverage_matrix[i] = normalize(coverage_matrix[i])\n",
    "\n",
    "    def get_score(state: List[int]) -> float:\n",
    "        theta_similarity = 1.\n",
    "        theta_coverage = 1.3\n",
    "        theta_compat = 1.\n",
    "        if not state:\n",
    "            return 0\n",
    "        coverage_score = get_state_coverage(state)   \n",
    "        similarity_score = candidates_similarity[state].mean().item()\n",
    "        if use_ordered_scores:            \n",
    "             compat_score = order_scores[state].mean().item()\n",
    "        else:            \n",
    "             compat_score = compat_scores[state].mean().item()\n",
    "        return theta_coverage*coverage_score + theta_similarity*similarity_score + theta_compat*compat_score\n",
    "\n",
    "    def get_expert_coverage(state):\n",
    "        # return self.coverage_matrix[:,state].sum(axis=1)\n",
    "        return coverage_matrix[:,state].max(axis=1)\n",
    "          \n",
    "    def get_state_coverage(state) -> float:\n",
    "        # print(\"State coverage for {}:\".format(state))\n",
    "        # print(get_expert_coverage(state))\n",
    "        return np.mean(get_expert_coverage(state))\n",
    "\n",
    "\n",
    "    superset = list(range(len(sents)))\n",
    "    pset = [list(x) for x in powerset(superset)]\n",
    "    pset_scores = [get_score(x) for x in pset]\n",
    "    best_cand = pset[np.argmax(pset_scores)]\n",
    "    print(\"Best candidates:\")\n",
    "    print(best_cand)\n",
    "    rc_sents = itemgetter(*best_cand)(sents)\n",
    "    if type(rc_sents)==tuple:\n",
    "        rc = list(rc_sents)\n",
    "    elif type(rc_sents)==str:\n",
    "        rc = [rc_sents]\n",
    "    else:\n",
    "        print(\"Bad return type!!\")\n",
    "    return rc, orig_similarity[best_cand].mean()\n",
    "\n",
    "\n",
    "def optimize_scene(doc,mat=None, emb_video=None, **kwargs):\n",
    "    mid = doc['movie_id']\n",
    "    elem = doc['scene_element']\n",
    "    emb_video = clip.clip_encode_video(mid,elem)\n",
    "    all_sents = doc['sentences']\n",
    "    rc = mdb.get_scene_from_collection(mid,elem,'s2_clsmdc')    \n",
    "    experts = flatten(rc['experts'].values())\n",
    "    rc = mdb.get_scene_from_collection(mid,elem,s2_compatibility_collection_name)  \n",
    "    all_compat_scores = rc['compat_scores']\n",
    "    n = len(all_sents)\n",
    "    rc_sents = n*[None]\n",
    "    mean_scores = n*[None]\n",
    "    for i in range(n):\n",
    "        rc_sents[i], mean_scores[i] = optimize_sents(emb_video,experts,all_sents[i],all_compat_scores[i], **kwargs)\n",
    "\n",
    "    return rc_sents, mean_scores\n",
    "    \n",
    "def run_pipeline(all_docs, target_collection_name=s2_with_compat_collection_name, **kwargs):\n",
    "    for doc in all_docs:\n",
    "        mid = doc['movie_id']\n",
    "        elem = doc['scene_element']\n",
    "        rc = mdb.get_scene_from_collection(mid,elem,target_collection_name)\n",
    "        if rc:\n",
    "            print(\"Results already exist for {}/{}\".format(mid,elem))\n",
    "            continue\n",
    "        print(\"Going forward with {}/{}\".format(mid,elem))\n",
    "\n",
    "        rc_sents, sim_scores = optimize_scene(doc,**kwargs)\n",
    "        rc_doc = {\n",
    "            'movie_id': mid,\n",
    "            'scene_element': elem,\n",
    "            'sentences': rc_sents,\n",
    "            'mean_scores': sim_scores,\n",
    "        }\n",
    "        query = \"INSERT {} INTO {}\".format(rc_doc,target_collection_name)\n",
    "        cursor = pipeline.db.aql.execute(query)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'FOR doc IN {} RETURN doc'.format(s2_compatibility_collection_name)\n",
    "cursor = pipeline.db.aql.execute(query)\n",
    "all_docs = sorted(list(cursor), key=lambda x:\"{}/{}\".format(x['movie_id'],x['scene_element']))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dafab0f5b0f2e0b482ce484a64bf4a63ea947b97362cb54784af04b5754b7b41"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
