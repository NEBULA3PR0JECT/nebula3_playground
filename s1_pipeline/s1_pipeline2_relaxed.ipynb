{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/notebooks/pipenv\")\n",
    "sys.path.insert(0, \"/notebooks/nebula3_vlm\")\n",
    "sys.path.insert(0, \"/notebooks/nebula3_database\")\n",
    "sys.path.insert(0, \"/notebooks/\")\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import bisect\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib\n",
    "import subprocess\n",
    "import re\n",
    "import tempfile\n",
    "import itertools\n",
    "import torch\n",
    "import spacy\n",
    "# import amrlib\n",
    "# import penman\n",
    "\n",
    "from typing import List, Tuple\n",
    "from operator import itemgetter \n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "from database.arangodb import DatabaseConnector\n",
    "from config import NEBULA_CONF\n",
    "from movie_db import MOVIE_DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIPELINE:\n",
    "    def __init__(self):\n",
    "        config = NEBULA_CONF()\n",
    "        self.db_host = config.get_database_host()\n",
    "        self.database = config.get_playground_name()\n",
    "        self.gdb = DatabaseConnector()\n",
    "        self.db = self.gdb.connect_db(self.database)\n",
    "\n",
    "pipeline = PIPELINE()\n",
    "mdb = MOVIE_DB()\n",
    "from vlm.clip_api import CLIP_API\n",
    "clip=CLIP_API('vit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst): return [x for l in lst for x in l]\n",
    "\n",
    "def compute_batch_scores(video_emb: torch.Tensor, texts: List[str], normalize=True, **kwargs) -> List[float]:    \n",
    "    emb_batch = clip.clip_batch_encode_text(texts, **kwargs)                           \n",
    "    return (video_emb.expand_as(emb_batch)*emb_batch).sum(dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "def compute_concat_score(image_emb: torch.Tensor, texts: List[str], join_on=',') -> float:\n",
    "    combined_text = \"\"\n",
    "    for t in [x.strip() for x in texts]:\n",
    "        if t[-1]=='.':\n",
    "            t = t[:-1]       \n",
    "        t+=join_on\n",
    "        t+=' '\n",
    "        combined_text+=t\n",
    "    print(\"Combined: \"+combined_text)\n",
    "    return torch.matmul(image_emb,mdmmt.encode_text(combined_text.strip()) )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_concept(c):\n",
    "    exp = re.compile(r\"^([a-zA-z]+)-?(\\d*)$\")\n",
    "    r = exp.match(c)\n",
    "    return r.group(1) if r else c\n",
    "\n",
    "class ConceptManager:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def ground_concept(concept):\n",
    "        return transform_concept(concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityManager:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "    def similarity(self, src, target):\n",
    "        rc = []\n",
    "        s1 = self.nlp(src)\n",
    "        s2 = self.nlp(target)\n",
    "        for w in s1:\n",
    "            if w.pos_ not in ['NOUN', 'ADJ', 'ADV', 'VERB', 'PROPN'] and len(s1)>1:\n",
    "                continue\n",
    "            rc.append(max([w.similarity(x) for x in s2]))\n",
    "        return np.mean(rc)\n",
    "        \n",
    "smanager = SimilarityManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = lambda x: np.exp(x)/sum(np.exp(x))\n",
    "normalize = lambda x: (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "class SubsetOptimization:\n",
    "    def __init__(self, video_emb, experts: List, candidates_strings: List[str], coverage_matrix = None):\n",
    "        self.video_emb = video_emb\n",
    "        self.initial_temp = 10\n",
    "        self.final_temp = .001\n",
    "        self.alpha = 0.01\n",
    "        self.theta = 0.9\n",
    "        self.theta1 = 0.5\n",
    "        self.remove_prob_temprature = 10.\n",
    "        self.expert_to_cover_temp = 1.\n",
    "        self.candidate_to_add_cover_temp = 10.\n",
    "        self.candidate_to_add_vlm_temp = 10.\n",
    "        self.reset_every = 5000\n",
    "        self.experts = experts\n",
    "        self.coverage_threshold = 3.0\n",
    "        self.candidates_strings = candidates_strings\n",
    "        print(\"Computing batch similarity...\")\n",
    "        self.candidates_similarity = compute_batch_scores(self.video_emb, self.candidates_strings)\n",
    "        print(\"Done\")\n",
    "        self.opt_results = []\n",
    "        self.smanager = SimilarityManager()\n",
    "\n",
    "        if coverage_matrix is not None:\n",
    "            self.coverage_matrix = coverage_matrix\n",
    "        else:\n",
    "            self.coverage_matrix = np.zeros([len(self.experts),len(self.candidates_strings)])\n",
    "            self.coverage_matrix[:] = np.nan\n",
    "            for i in range(len(experts)):\n",
    "                for j in range(len(candidates_strings)):\n",
    "                    self.coverage_matrix[i][j]=self.concept_similarity(self.experts[i],self.candidates_strings[j])\n",
    "        self.max_size = int(len(self.experts)*1.5)\n",
    "\n",
    "    def concept_similarity(self, concept, sent):        \n",
    "        # return max(self.smanager.similarity(concept,sent))\n",
    "        return self.smanager.similarity(concept,sent)\n",
    "\n",
    "    def get_coverage(self,i,j):        \n",
    "        if np.isnan(self.coverage_matrix[i][j]):\n",
    "            self.coverage_matrix[i][j] = self.concept_similarity(self.experts[i],self.candidates_strings[j])\n",
    "        return self.coverage_matrix[i][j]\n",
    "\n",
    "    # Return, for each expert, the -sum total- of how much it is covered by the state.\n",
    "\n",
    "    def get_expert_coverage(self,state):\n",
    "        return self.coverage_matrix[:,state].sum(axis=1)\n",
    "        # return self.coverage_matrix[:,state].max(axis=1)\n",
    "\n",
    "    def get_state_coverage(self,state) -> float:\n",
    "        # print(\"State coverage for {}:\".format(state))\n",
    "        # print(self.get_expert_coverage(state))\n",
    "        return np.mean(self.get_expert_coverage(state))\n",
    "\n",
    "    # def get_state_coverage(self, state: List[int]) -> float:\n",
    "    #     experts_coverage = [max([self.get_coverage(i,j) for j in state]) for i in range(len(self.experts))]    # A list of partial coverege        \n",
    "    #     return sum(experts_coverage) / len(self.experts)\n",
    "\n",
    "    def get_score(self, state: List[int]) -> float:\n",
    "        if not state:\n",
    "            return 0\n",
    "        coverage_score = self.get_state_coverage(state)   \n",
    "        similarity_score = self.candidates_similarity[state].mean().item()\n",
    "        return (1-self.theta)*coverage_score + self.theta*similarity_score\n",
    "\n",
    "\n",
    "    def prob_to_remove(self, state):\n",
    "        cover = self.get_state_coverage(state)\n",
    "        relative_cover = min(max(cover / self.coverage_threshold,0),1)\n",
    "        rc = np.power(relative_cover,4)\n",
    "        # print(\"Cover, relative cover, prob to remove: {}, {}, {}\".format(cover, relative_cover, rc))\n",
    "        return rc\n",
    "\n",
    "\n",
    "    def candidate_to_add_probs(self, expert_to_cover, anti_state):\n",
    "        add_cover = normalize(self.coverage_matrix[expert_to_cover][anti_state])\n",
    "        add_similarity = normalize(self.candidates_similarity[anti_state])\n",
    "        # print(\"add_cover:\")\n",
    "        # print(add_cover)\n",
    "        # print(\"add_similarity:\")\n",
    "        # print(add_similarity)\n",
    "        probs = softmax(add_cover*self.candidate_to_add_cover_temp + add_similarity*self.candidate_to_add_vlm_temp)\n",
    "        return probs\n",
    "     \n",
    "    # state here is assumed (and guaranteed on return) to be -sorted-\n",
    "    def get_candidate(self, state: List[int]) -> List[int]:\n",
    "        def compute_state_arrays(s):\n",
    "            s_score = self.candidates_similarity[s]\n",
    "            s_coverage = self.coverage_matrix.mean(axis=0)[s]\n",
    "            s_max_coverage = self.coverage_matrix.max(axis=0)[s]\n",
    "            s_fitscore = (1-self.theta1)*s_coverage+self.theta1*s_score\n",
    "            # print(\"fitscore: {}\".format(s_fitscore))\n",
    "            return (s_score,s_coverage,s_max_coverage,s_fitscore)\n",
    "\n",
    "        if not state:\n",
    "            # print(\"Empty state\")\n",
    "            return [random.randint(0,len(self.candidates_strings)-1)]\n",
    "            \n",
    "        rc = state.copy()\n",
    "        s = np.array(state)\n",
    "        s_score, s_coverage, s_max_coverage, s_fitscore = compute_state_arrays(s)\n",
    "               \n",
    "        if len(state) == self.max_size:\n",
    "            print(\"Maximum state size, removing\")\n",
    "            idx = np.argmin(s_fitscore)\n",
    "            del rc[idx]\n",
    "            return rc\n",
    "            \n",
    "        remove_sentence = random.random()<self.prob_to_remove(state)      \n",
    "        # print(\"coverage of {} is {}, remove?{}\".format(state,self.get_state_coverage(state),remove_sentence))\n",
    "        if remove_sentence:             # We decide to remove a sentence from the set\n",
    "            # print(\"Removing\")\n",
    "            probs = softmax(-s_fitscore*self.remove_prob_temprature)\n",
    "            # print(probs)\n",
    "            idx = np.random.multinomial(1,probs).argmax()\n",
    "            # print(\"removing index: {}\".format(idx))\n",
    "            del rc[idx]                   \n",
    "        else:                           # Add a sentence from the outside\n",
    "            # print(\"Adding\")\n",
    "            anti_state = []\n",
    "            for i in range(len(self.candidates_strings)):\n",
    "                if not i in state:\n",
    "                    anti_state.append(i)\n",
    "            s1 = np.array(anti_state)\n",
    "            s1_score, s1_coverage, s1_max_coverage, s1_fitscore = compute_state_arrays(s1)\n",
    "            # Pick an expert to try and cover\n",
    "            expert_coverage = self.get_expert_coverage(s)\n",
    "            # print(\"expert coverage when adding:\")\n",
    "            # print(list(zip(self.experts,expert_coverage)))\n",
    "            probs = softmax(-expert_coverage*self.expert_to_cover_temp)         # Coverage is in (0,1), so we use low temprature\n",
    "            # print(probs)\n",
    "            expert_to_cover = np.random.multinomial(1,probs).argmax()\n",
    "            # print(\"Trying to cover expert: {}\".format(self.experts[expert_to_cover]))\n",
    "            probs = self.candidate_to_add_probs(expert_to_cover,s1)\n",
    "            # probs = softmax(self.coverage_matrix[expert_to_cover][s1]*10)\n",
    "            idx_to_add = np.random.multinomial(1,probs).argmax()\n",
    "            bisect.insort(rc,anti_state[idx_to_add])\n",
    "            \n",
    "        return rc\n",
    "\n",
    "    def temp_schedule(self,i):\n",
    "        schedule = [(5000,0.5), (15000,0.1), (25000,0.01), (35000,0.005), (45000,self.final_temp)]\n",
    "        if i<schedule[0][0]:\n",
    "            return schedule[0][1]\n",
    "        if i>=schedule[-1][0]:\n",
    "            return schedule[-1][1]\n",
    "        for j in range(len(schedule)):\n",
    "            if i<schedule[j+1][0]:\n",
    "                break\n",
    "        start = schedule[j][0]\n",
    "        end = schedule[j+1][0]\n",
    "        start_val = schedule[j][1]\n",
    "        end_val = schedule[j+1][1]\n",
    "\n",
    "        # if i > 20:\n",
    "        #     return self.final_temp\n",
    "\n",
    "        return ((i-start)/(end-start))*(end_val-start_val)+start_val         \n",
    "\n",
    "    def get_scored_permutations(self, k):\n",
    "        n = len(self.candidates)\n",
    "        return [(x,self.get_score(list(x))) for x in itertools.permutations(range(n),k)]\n",
    "\n",
    "    def reset(self):\n",
    "        return max(self.opt_results,key=lambda x:x[1])[0]\n",
    "    \n",
    "        \n",
    "    def simulated_annealing(self, initial_state, clear_prev = False, reset_every = None):\n",
    "        current_temp = self.initial_temp\n",
    "        i = 0\n",
    "        if clear_prev:\n",
    "            self.opt_results = []\n",
    "        if not reset_every:\n",
    "            reset_every = self.reset_every\n",
    "\n",
    "       # Start by initializing the current state with the initial state\n",
    "        current_state = initial_state\n",
    "        curr_score = self.get_score(initial_state)\n",
    "\n",
    "        while current_temp > self.final_temp:\n",
    "            if i % reset_every == 0 and i>0:                \n",
    "                next_cand = self.reset()\n",
    "                print(\"Reset to state: {}\".format(next_cand))\n",
    "            else:\n",
    "                next_cand = self.get_candidate(current_state)            \n",
    "            next_score = self.get_score(next_cand)\n",
    "\n",
    "            # print(\"current score: {} ({}). Candidate score: {} ({})\".format(curr_score,current_state,next_score,next_cand))\n",
    "\n",
    "            # Check if next_cand is best so far\n",
    "            score_diff = next_score - curr_score\n",
    "\n",
    "            # if the new solution is better, accept it\n",
    "            move = False\n",
    "            if score_diff > 0:\n",
    "                move = True\n",
    "            # if the new solution is not better, accept it with a probability of e^(-cost/temp)\n",
    "            else:\n",
    "                # print(\"chance to move (from score_diff {}): {}\".format(score_diff,math.exp(score_diff / current_temp)))\n",
    "                move = random.uniform(0, 1) < math.exp(score_diff / current_temp)                    \n",
    "            if move:\n",
    "                current_state = next_cand\n",
    "                curr_score = next_score\n",
    "                self.opt_results.append((current_state,curr_score))\n",
    "            # decrement the temperature\n",
    "            current_temp = self.temp_schedule(i)\n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(\"i: {}\".format(i))            \n",
    "\n",
    "        return self.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_scene(doc,mat=None):\n",
    "    rc = mdb.get_scene_from_collection(doc['movie_id'],doc['scene_element'],'s1_lsmdc')\n",
    "    experts = flatten(rc['experts'].values())\n",
    "    sents = list(set(doc['combined_sentences']))\n",
    "    if not sents:\n",
    "        print(\"Empty list of sentences, aborting!\")\n",
    "        return []\n",
    "    emb_video = clip.clip_encode_video(doc['movie_id'],doc['scene_element'])\n",
    "    optim = SubsetOptimization(emb_video, experts, sents, coverage_matrix=mat)\n",
    "    rc = optim.simulated_annealing([], reset_every=15000)\n",
    "    coverage = list(zip(optim.experts, optim.get_expert_coverage(rc)))\n",
    "    print(\"Coverage:\")\n",
    "    print(coverage)\n",
    "    print(\"Similarity:\")\n",
    "    print(\"Mean of result: {}\".format(optim.candidates_similarity[rc].mean()))\n",
    "    list(zip(itemgetter(*rc)(optim.candidates_strings),optim.candidates_similarity[rc]))\n",
    "    return list(itemgetter(*rc)(optim.candidates_strings)), optim\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'FOR doc IN s1_pipeline_results_phase2 RETURN doc'\n",
    "cursor = pipeline.db.aql.execute(query)\n",
    "all_docs = list(cursor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc = clip.clip_batch_encode_text([\"This is the first sentence\"])\n",
    "rc = clip.clip_batch_encode_text([\"This is the first sentence\", \"I see an elephant\", \"One last drink to go please\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9995, 1.0010, 1.0010], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.norm(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc1 = clip.clip_encode_text(\"This is the first sentence\")\n",
    "rc1.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ec2-18-159-140-240.eu-central-1.compute.amazonaws.com:7000/static/development/1038_The_Great_Gatsby_00_56_48_259-00_56_50_126.mp4\n",
      "23.976023976023978\n",
      "Movie info: {'arango_id': 'Movies/114207499', 'description': '1038_The_Great_Gatsby_00_56_48_259-00_56_50_126', 'fps': 23, 'width': 1920, 'height': 1080, 'last frame': 300, 'movie_id': 'd5d5f3307bef4d23aedb07f6c1f65b20', 'mdfs': [[2, 24, 46]], 'scene_elements': [[0, 48]]}\n",
      "fn path: /tmp/file.mp4\n",
      "/tmp/file.mp4\n",
      "Scene:  0\n"
     ]
    }
   ],
   "source": [
    "doc = all_docs[1]\n",
    "mid = doc['movie_id']\n",
    "elem = doc['scene_element']\n",
    "\n",
    "rc2 = clip.clip_encode_video(mid,elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ec2-18-159-140-240.eu-central-1.compute.amazonaws.com:7000/static/development/1038_The_Great_Gatsby_00_56_48_259-00_56_50_126.mp4\n",
      "23.976023976023978\n",
      "Movie info: {'arango_id': 'Movies/114207499', 'description': '1038_The_Great_Gatsby_00_56_48_259-00_56_50_126', 'fps': 23, 'width': 1920, 'height': 1080, 'last frame': 300, 'movie_id': 'd5d5f3307bef4d23aedb07f6c1f65b20', 'mdfs': [[2, 24, 46]], 'scene_elements': [[0, 48]]}\n",
      "fn path: /tmp/file.mp4\n",
      "/tmp/file.mp4\n",
      "Scene:  0\n",
      "Computing batch similarity...\n",
      "Done\n",
      "i: 1000\n",
      "i: 2000\n",
      "i: 3000\n",
      "i: 4000\n",
      "i: 5000\n",
      "i: 6000\n",
      "i: 7000\n",
      "i: 8000\n",
      "i: 9000\n",
      "i: 10000\n",
      "i: 11000\n",
      "i: 12000\n",
      "i: 13000\n",
      "i: 14000\n",
      "i: 15000\n",
      "Reset to state: [17, 43, 46, 92, 102]\n",
      "i: 16000\n",
      "i: 17000\n",
      "i: 18000\n",
      "i: 19000\n",
      "i: 20000\n",
      "i: 21000\n",
      "i: 22000\n",
      "i: 23000\n",
      "i: 24000\n",
      "i: 25000\n",
      "i: 26000\n",
      "i: 27000\n",
      "i: 28000\n",
      "i: 29000\n",
      "i: 30000\n",
      "Reset to state: [17, 43, 46, 92, 102]\n",
      "i: 31000\n",
      "i: 32000\n",
      "i: 33000\n",
      "i: 34000\n",
      "i: 35000\n",
      "i: 36000\n",
      "i: 37000\n",
      "i: 38000\n",
      "i: 39000\n",
      "i: 40000\n",
      "i: 41000\n",
      "i: 42000\n",
      "i: 43000\n",
      "i: 44000\n",
      "i: 45000\n",
      "Reset to state: [17, 43, 46, 92, 102]\n",
      "Coverage:\n",
      "[('tense', 2.8719094693660736), ('white suit', 4.116936177015305), ('orange tie', 4.364342242479324), ('sits', 4.178575038909912), ('lady', 3.7199745774269104), ('holds a cup of coffee', 4.442682137091954), ('waiter', 1.8395828306674957), ('green jacket', 5.0), ('flowers', 2.9702543020248413)]\n",
      "Similarity:\n",
      "Mean of result: 0.247314453125\n"
     ]
    }
   ],
   "source": [
    "rc, optim_local = optimize_scene(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The man in the suit and tie is looking tense as he sits next to a woman in a green jacket.',\n",
       "  'The man in the suit and tie sits and holds a cup of coffee while a woman in a green jacket stands next to him.',\n",
       "  'The man in the suit and tie sits and holds a cup of coffee as he looks at the woman in the green jacket.',\n",
       "  'The man in the suit and tie holds a cup of coffee and looks at the green jacket with flowers on it.',\n",
       "  \"The man in the suit and tie looks tense as he holds a cup of coffee in his hand. He's standing next to a woman who's wearing a green jacket.\"],\n",
       " 5)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc, len(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, True, True, False] 108 0.2147 0.26\n",
      "[0.2448 0.2455 0.2462 0.257  0.26  ]\n"
     ]
    }
   ],
   "source": [
    "ind = optim_local.candidates_similarity.argsort()[-5:]\n",
    "cands = np.array(optim_local.candidates_strings)[ind]\n",
    "scores = optim_local.candidates_similarity[ind]\n",
    "all_top = list(zip(cands,scores))\n",
    "print([x in list(cands) for x in rc], len(optim_local.candidates_strings), optim_local.candidates_similarity.mean(), optim_local.candidates_similarity.max())\n",
    "print(optim_local.candidates_similarity[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in all_docs:\n",
    "    mid = doc['movie_id']\n",
    "    elem = doc['scene_element']\n",
    "    rc = nre.get_scene_from_collection(mid,elem,'s1_pipeline_results_final')\n",
    "    if rc:\n",
    "        print(\"Results already exist for {}/{}\".format(mid,elem))\n",
    "        continue\n",
    "    print(\"Going forward with {}/{}\".format(mid,elem))\n",
    "    rc = optimize_scene(doc)\n",
    "    rc_doc = {\n",
    "        'movie_id': mid,\n",
    "        'scene_element': elem,\n",
    "        'sentences': rc\n",
    "    }\n",
    "    query = \"INSERT {} INTO s1_pipeline_results_final\".format(rc_doc)\n",
    "    cursor = nre.db.aql.execute(query)    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dafab0f5b0f2e0b482ce484a64bf4a63ea947b97362cb54784af04b5754b7b41"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nebula')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
